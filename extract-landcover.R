## ------------------------------------------------------- ##
      # Silica WG - Extract Spatial Data - Land Cover
## ------------------------------------------------------- ##
# Written by:
## Nick J Lyon

# Purpose:
## Using the watershed shapefiles created in "id-watershed-polygons.R"
## Extract the following data: LAND COVER

## ------------------------------------------------------- ##
                    # Housekeeping ----
## ------------------------------------------------------- ##

# Read needed libraries
# install.packages("librarian")
librarian::shelf(tidyverse, sf, stars, terra, exactextractr, NCEAS/scicomptools)

# Clear environment
rm(list = ls())

# Identify path to location of shared data
(path <- scicomptools::wd_loc(local = F, remote_path = file.path('/', "home", "shares", "lter-si", "si-watershed-extract")))

# Load in site names with lat/longs
sites <- read.csv(file = file.path(path, "site-coordinates", 'silica-coords_ACTUAL.csv'))

# Check it out
dplyr::glimpse(sites)

# Grab the shapefiles the previous script (see PURPOSE section) created
sheds <- sf::st_read(dsn = file.path(path, "site-coordinates", "silica-watersheds.shp"))

# Check that out
dplyr::glimpse(sheds)

# Clean up environment
rm(list = setdiff(ls(), c('path', 'sites', 'sheds')))

## ------------------------------------------------------- ##
# Land Cover - Luquillo Processing ----
## ------------------------------------------------------- ##

# Prepare an sf object of just this LTER's watershed shapes
luq_sf <- sheds %>%
  filter(LTER == "LUQ") %>%
  # And transformed to the LC raster's CRS
  st_transform(crs = 5070)

# Read in the raster
pr_rast <- terra::rast("extracted-data/raw-landcover-data/NLCD-PuertoRico-2001/NLCD_PuertoRico_2001_raw.img")

# Make sure CRS are same
crs(pr_rast)
st_crs(luq_sf)

# Plot that to see how it looks
plot(pr_rast, axes = T, reset = F)
plot(luq_sf["uniqueID"], add = T)

# Extract data from the raster using the sf object
luq_lc_list <- exactextractr::exact_extract(x = pr_rast, y = luq_sf, include_cols = c("LTER", "uniqueID"))
str(luq_lc_list)

# Grab the NLCD index that connects integer codes to meaningful categories
nlcd_index <- read.csv("extracted-data/raw-landcover-data/NLCD_index.csv")
head(nlcd_index)

# Process our landcover information
luq_lc_actual <- luq_lc_list %>%
  # Make a dataframe from the selected list columns
  map_dfr(select, c(LTER, uniqueID, value)) %>%
  # Rename the land cover column
  rename(nlcd_code = value) %>%
  # Group by code, LTER, and uniqueID
  group_by(LTER, uniqueID, nlcd_code) %>%
  # Count pixels per code within the watershed
  summarise(cover_pixel_ct = n()) %>%
  # Grab all of the contents of the index
  left_join(nlcd_index, by = "nlcd_code") %>%
  # Keep only columns that we want
  select(LTER, uniqueID, nlcd_category, cover_pixel_ct) %>%
  # Export as dataframe
  as.data.frame()

# Check it out
head(luq_lc_actual)

# Save this out
write.csv(luq_lc_actual, "extracted-data/raw-landcover-data/NLCD-LUQ-landcover.csv", row.names = F)

# Clean up environment
rm(list = setdiff(ls(), c('path', 'sites', 'sheds')))

## ------------------------------------------------------- ##
# Land Cover - Arctic Processing ----
## ------------------------------------------------------- ##

# Prepare an sf object of just this LTER's watershed shapes in the correct CRS
arc_sf <- sheds %>%
  filter(LTER == "ARC") %>%
  st_transform(crs = 5070)

# Read in the raster
ak_rast <- terra::rast("extracted-data/raw-landcover-data/NLCD-ARC-bbox-2016/NLCD_ARC_bbox_2016_raw.tiff")

# Make sure CRS are same
crs(ak_rast)
st_crs(arc_sf)

# Plot that to see how it looks
plot(ak_rast, axes = T, reset = F)
## For some reason the conversion made the whole world of blank no-data show up so just ignore that
plot(arc_sf["uniqueID"], add = T)

# Extract data from the raster using the sf object
arc_lc_list <- exactextractr::exact_extract(x = ak_rast, y = arc_sf, include_cols = c("LTER", "uniqueID"))
str(arc_lc_list)

# Grab the NLCD index that connects integer codes to meaningful categories
nlcd_index <- read.csv("extracted-data/raw-landcover-data/NLCD_index.csv")
head(nlcd_index)

# Process our landcover information
arc_lc_actual <- arc_lc_list %>%
  # Make a dataframe from the selected list columns
  map_dfr(select, c(LTER, uniqueID, value)) %>%
  # Rename the land cover column
  rename(nlcd_code = value) %>%
  # Group by code, LTER, and uniqueID
  group_by(LTER, uniqueID, nlcd_code) %>%
  # Count pixels per code within the watershed
  summarise(cover_pixel_ct = n()) %>%
  # Grab all of the contents of the index
  left_join(nlcd_index, by = "nlcd_code") %>%
  # Keep only columns that we want
  select(LTER, uniqueID, nlcd_category, cover_pixel_ct) %>%
  # Export as dataframe
  as.data.frame()

# Check it out
head(arc_lc_actual)

# Save this out
write.csv(arc_lc_actual, "extracted-data/raw-landcover-data/NLCD-ARC-landcover.csv", row.names = F)

# Clean up environment
rm(list = setdiff(ls(), c('path', 'sites', 'sheds')))

## ------------------------------------------------------- ##
# Land Cover - Continental USA Processing ----
## ------------------------------------------------------- ##
# This includes AND, HBR, KRR, LMP, LUQ, MCM, NWT, Sagehen, & UMR

# Because the process is the same for extracting data for all of our LTERs, let's do it in a for loop 

# Also, this for loop has a grouped `dplyr::summarize` call so we'll turn off the annoying dialogue that it returns before going into the loop
options(dplyr.summarise.inform = F)

# Grab the NLCD index that connects integer codes to meaningful categories
nlcd_index <- read.csv("extracted-data/raw-landcover-data/NLCD_index.csv")
head(nlcd_index)

# Now time for the for loop (the `setdiff` is needed to split off the ones that we either already have or will need to process via a different workflow)
for(lter in setdiff(unique(sites$LTER), c("ARC", "LUQ", "GRO", "MCM"))){
  
  # Print a start message (for diagnostic purposes)
  print(paste0(lter, " processing begun at ", Sys.time()))
  
  # Read in the pre-transformed raster
  bbox <- terra::rast(paste0("extracted-data/raw-landcover-data/NLCD-", lter ,"-bbox-2019/NLCD_", lter, "_bbox_2019_raw.tiff"))
  
  # Prepare the relevant subset of the sf object of our watersheds
  sf <- sheds %>%
    filter(LTER == lter) %>%
    st_transform(crs = 5070)
  
  # Extract data from the raster using the sf object
  lc_list <- exactextractr::exact_extract(x = bbox, y = sf, include_cols = c("LTER", "uniqueID"))
  
  # Now we have a smidge more processing to do before we should export
  lc_actual <- lc_list %>%
    # Make a single dataframe from the selected columns of each list element
    map_dfr(select, c(LTER, uniqueID, value)) %>%
    # Rename the land cover column
    rename(nlcd_code = value) %>%
    # Group by code, LTER, and uniqueID
    group_by(LTER, uniqueID, nlcd_code) %>%
    # Count pixels per code within the watershed
    summarise(cover_pixel_ct = n()) %>%
    # Grab all of the contents of the index
    left_join(nlcd_index, by = "nlcd_code") %>%
    # Keep only columns that we want
    select(LTER, uniqueID, nlcd_category, cover_pixel_ct) %>%
    # Export as dataframe
    as.data.frame()
  
  # This object is the one we want so save it to the server as a csv
  write.csv(lc_actual, paste0("extracted-data/raw-landcover-data/NLCD-", lter, "-landcover.csv"), row.names = F)
  
  # End with a conclusion message to mirror the start message
  print(paste0(lter, " processing completed at ", Sys.time()))  }
## Note that the part of the loop handling UMR takes a few minutes (3 min on NCEAS server)

# Clean up environment
rm(list = setdiff(ls(), c('path', 'sites', 'sheds')))

## ------------------------------------------------------- ##
# Land Cover - Great Rivers Observatory Processing ----
## ------------------------------------------------------- ##
# Data source https://earthexplorer.usgs.gov/scene/metadata/full/5e83a1d8eecc8bb5/GLCCGBE20/

# Get relevant part of watershed polygons
gro_sf <- sheds %>%
  filter(LTER == "GRO")

# Read in data
## gbigbpgeo20 = International Geosphere Biosphere Programme Land Cover Classification  (IGBP)
globe <- terra::rast("extracted-data/raw-landcover-data/MODIS_GLCC_Global_1992/gbigbpgeo20.tif")

# Exploratory plot
plot(globe)

# Crop to band of interest (all longitudes but only northern-most latitudes)
ext(globe)
globe_crop <- terra::crop(globe, terra::ext(-179.999999999967, 179.999985600033,
                                            55, 89.9999999999667))

# Plot cropped version
plot(globe_crop)

# Check CRS
crs(globe_crop)
st_crs(gro_sf)
## Looks like they match right out of the box!

# Try to extract information
gro_lc_list <- exactextractr::exact_extract(x = globe_crop, y = gro_sf, include_cols = c("LTER", "uniqueID"))
str(gro_lc_list)

# Read in IGBP index
## Created manually from description here
## www.usgs.gov/media/files/global-land-cover-characteristics-data-base-readme-version2
igbp_index <- read.csv("extracted-data/raw-landcover-data/IGBP_index.csv")
head(igbp_index)

# Process this out of list form
gro_lc <- gro_lc_list %>%
  # Make a dataframe from the selected list columns
  map_dfr(select, c(LTER, uniqueID, value)) %>%
  # Group by LTER and uniqueID
  group_by(LTER, uniqueID, value) %>%
  # Count instances of each pixel category
  summarise(cover_pixel_ct = n()) %>%
  # Bring in category names
  rename(igbp_code = value) %>%
  left_join(igbp_index, by = "igbp_code") %>%
  # Fix weird spacing issue
  mutate(igbp_category = str_sub(igbp_category, 2, nchar(igbp_category))) %>%
  # Keep only desired columns
  select(LTER, uniqueID, igbp_category, cover_pixel_ct)

# Check it
unique(gro_lc$igbp_category)
head(gro_lc)

# Export this as is so that we can standardize cover categories between different sources of land cover data
write.csv(gro_lc, "extracted-data/raw-landcover-data/IGBP-GRO-landcover.csv", na = '', row.names = F)

# Clean up environment
rm(list = setdiff(ls(), c('path', 'sites', 'sheds')))

## ------------------------------------------------------- ##
# Land Cover - Combine Watersheds ----
## ------------------------------------------------------- ##

# We have--at this point--successfully summarized land cover data from most of our LTERs; now we want to combine our LTER-specific csvs into one 'main' variant

# Set a counter and empty list
nlcd_list <- list()
k <- 1

# Read in the csvs (not for MCM or GRO because we don't have them yet and when we do they'll come from different sources)
for(lter in setdiff(unique(sites$LTER), c("GRO", "MCM"))){
  
  # Grab a csv
  nlcd_data <- read.csv(paste0("extracted-data/raw-landcover-data/NLCD-", lter, "-landcover.csv"))
  
  # Add it to the kth element of our list
  nlcd_list[[k]] <- nlcd_data
  
  # Advance the counter
  k <- k + 1 }

# Switch from list to dataframe
nlcd_data <- nlcd_list %>%
  map_dfr(select, c(LTER, uniqueID, nlcd_category, cover_pixel_ct))
head(nlcd_data)

# Read in GRO land cover (from different source!)
gro_data <- read.csv("extracted-data/raw-landcover-data/IGBP-GRO-landcover.csv")
head(gro_data)

# Combine the two dataframes (without standardizing cover names)
lc_unmod <- nlcd_data %>%
  # Use bind_rows to account for difference in category names
  dplyr::bind_rows(gro_data) %>%
  # Move the GRO's category column to be next to the other one
  relocate(igbp_category, .before = cover_pixel_ct) %>%
  # Make a single column for both types of category
  mutate(cover_category = coalesce(nlcd_category, igbp_category)) %>%
  # Remove any NAs
  filter(!is.na(cover_category)) %>%
  # Standardize casing/special character use in that combined column
  mutate(cover_category = tolower(gsub("\\/| |\\-", "_", cover_category))) %>%
  # Remove unneeded columns
  select(-nlcd_category, -igbp_category) %>%
  # Calculate total pixels per stream
  group_by(LTER, uniqueID) %>%
  mutate(total_pixels = sum(cover_pixel_ct, na.rm = T)) %>%
  # And pivot to wide format
  pivot_wider(id_cols = -cover_category:-cover_pixel_ct,
              names_from = cover_category,
              values_from = cover_pixel_ct)

# Integrate the two dataframes (for real)
lc_data <- nlcd_data %>%
  # Use bind_rows to account for difference in category names
  dplyr::bind_rows(gro_data) %>%
  # Move the GRO's category column to be next to the other one
  relocate(igbp_category, .before = cover_pixel_ct) %>%
  # Make a single column for both types of category
  mutate(cover_category = coalesce(nlcd_category, igbp_category)) %>%
  # Remove any NAs
  filter(!is.na(cover_category)) %>%
  # Collapse seemingly synonymous categories into one another (and standardize casing/special characters)
  dplyr::mutate(
    cover_category = case_when(
      # Some additional combination is possible but these are the "safe" changes in my (Nick's) opinion
      ## NLCD category simplification
      nlcd_category == "Barren_Land" ~ "barren_land",
      nlcd_category == "Perennial_Ice_Snow" ~ "barren_land",
      nlcd_category == "Cultivated_Crops" ~ "cultivated_crops",
      nlcd_category == "Deciduous_Forest" ~ "deciduous_forest",
      nlcd_category == "Developed_High_Intensity" ~ "low_medium_intensity_developed",
      nlcd_category == "Developed_Low_Intensity" ~ "low_medium_intensity_developed",
      nlcd_category == "Developed_Medium_Intensity" ~ "low_medium_intensity_developed",
      nlcd_category == "Developed_Open_Space" ~ "low_medium_intensity_developed",
      nlcd_category == "Evergreen_Forest" ~ "evergreen_forest",
      nlcd_category == "Grassland_Herbaceous" ~ "grassland",
      nlcd_category == "Sedge_Herbaceous" ~ "grassland",
      nlcd_category == "Mixed_Forest" ~ "mixed_forest",
      nlcd_category == "Open_Water" ~ "open_water",
      nlcd_category == "Pasture_Hay" ~ "pasture_hay",
      nlcd_category == "Dwarf_Shrub" ~ "shrubland",
      nlcd_category == "Shrub_Scrub" ~ "shrubland",
      nlcd_category == "Emergent_Herbaceous_Wetlands" ~ "wetland",
      nlcd_category == "Woody_Wetlands" ~ "wetland",
      ## IGBP category simplification
      igbp_category == "Barren or Sparsely Vegetated " ~ "barren_land",
      igbp_category == "Snow and Ice" ~ "barren_land",
      igbp_category == "Cropland/Natural Vegetation Mosaic" ~ "cultivated_crops",
      igbp_category == "Croplands" ~ "cultivated_crops",
      igbp_category == "Deciduous Needleleaf Forest" ~ "deciduous_needleaf_forest",
      igbp_category == "Urban and Built-Up" ~ "low_medium_intensity_developed",
      igbp_category == "Evergreen Needleleaf Forest" ~ "evergreen_forest",
      igbp_category == "Grasslands" ~ "grassland",
      igbp_category == "Savannas" ~ "grassland",
      igbp_category == "Woody Savannas" ~ "grassland", # *NOTE JUDGMENT CALL*
      igbp_category == "Mixed Forest" ~ "mixed_forest",
      igbp_category == "Water Bodies" ~ "open_water",
      igbp_category == "Closed Shrublands" ~ "shrubland",
      igbp_category == "Open Shrublands" ~ "shrubland",
      igbp_category == "Permanent Wetlands" ~ "wetland",
      # _category == "" ~ "",
      T ~ as.character(cover_category) ) ) %>%
  # Re-summarize within our new categories
  ## Note that this drops the original category columns so you'd need to go back to find those
  group_by(LTER, uniqueID, cover_category) %>%
  summarise(cover_pixel_ct = sum(cover_pixel_ct)) %>%
  # Group by LTER and uniqueID (looking across categories within watersheds)
  group_by(LTER, uniqueID) %>%
  # Count the total pixels and get percent from that
  mutate(
    total_pixels = sum(cover_pixel_ct),
    perc_cover = round((cover_pixel_ct / total_pixels) * 100, digits = 2)
  ) %>%
  # Slim down to only needed columns (drops unspecified cols implicitly)
  select(LTER, uniqueID, cover_category, perc_cover)

# Look at it
head(lc_data)

# As with lithology, we need to process in two directions
## Wide format with all percents
lc_wide <- lc_data %>%
  pivot_wider(id_cols = c(LTER, uniqueID),
              names_from = cover_category,
              values_from = perc_cover)

## Second: get the *majority* cover for each watershed
lc_major <- lc_data %>%
  # Filter to only max of each cover type per uniqueID & LTER
  group_by(LTER, uniqueID) %>%
  filter(perc_cover == max(perc_cover)) %>%
  # Remove the percent total
  dplyr::select(-perc_cover) %>%
  # Get the columns into wide format where the column name and value are both whatever the dominant cover was
  pivot_wider(id_cols = c(LTER, uniqueID),
              names_from = cover_category,
              values_from = cover_category) %>%
  # Paste all the non-NAs (i.e., the dominant rocks) into a single column
  unite(col = major_cover, -LTER:-uniqueID, na.rm = T, sep = "; ")

# Now attach the major rocks to the wide format one
lc_actual <- lc_wide %>%
  left_join(lc_major, by = c("LTER", "uniqueID")) %>%
  relocate(major_cover, .after = uniqueID)

# Examine
head(lc_actual)

## ------------------------------------------------------- ##
# Land Cover - Export ----
## ------------------------------------------------------- ##

# Let's get ready to export
cover_export <- sites %>%
  left_join(lc_actual, by = c("LTER", "uniqueID"))

# Check it out
head(cover_export)

# Export this csv
write.csv(x = cover_export,
          file = "extracted-data/SilicaSites_landcover.csv",
          na = '', row.names = F)

# And export the unmodified variant too
write.csv(x = lc_unmod,
          file = "extracted-data/SilicaSites_landcover_raw_categories.csv",
          na = '', row.names = F)

# Clean up environment
rm(list = setdiff(ls(), c('path', 'sites', 'sheds')))

## ------------------------------------------------------- ##
# Combine Lithology and Land Cover ----
## ------------------------------------------------------- ##

# With both lithology and land cover data in hand, let's make a single 'one-stop shop' dataframe containing both

# Read in both
rocks <- read.csv("extracted-data/SilicaSites_litho.csv")
cover <- read.csv("extracted-data/SilicaSites_landcover.csv")

# Get a vector of shared column names
shared_cols <- intersect(names(rocks), names(cover))
shared_cols

# Combine and process the two
combo <- cover %>%
  # Left join by shared columns
  left_join(rocks, by = shared_cols) %>%
  # Re-order columns
  relocate(major_rock, .after = major_cover) %>%
  # We want to fix some column names but its faster to do this via long format
  pivot_longer(cols = volcanic:plutonic,
               names_to = "rock_types", values_to = "rocks_perc") %>%
  # Fix the column names by:
  mutate(
    ## Adding prefix "rocks_" to all columns (this will help differentiate between rock and cover data that might otherwise be ambiguous)
    rock_types = paste0("rocks_", rock_types) ) %>%
  # Pivot back to wide format
  pivot_wider(names_from = rock_types,
              values_from = rocks_perc) %>%
  # Now do the same for the cover categories
  pivot_longer(cols = deciduous_forest:pasture_hay,
               names_to = "cover_types", values_to = "cover_perc") %>%
  mutate(cover_types = paste0("cover_", cover_types)) %>%
  pivot_wider(names_from = cover_types,
              values_from = cover_perc) %>%
  # McMurdo uses expert knowledge rather than extracted information (for now)
  mutate(major_rock = ifelse(LTER == "MCM", yes = "glacial_drift", no = major_rock),
         major_cover = ifelse(LTER == "MCM", yes = "barren_land", no = major_cover)) %>%
  # And let's also add some more 'data source' information to this for later use
  mutate(
    rockSource = ifelse(LTER == "MCM", yes = "Expert knowledge",
                        no = "Hartmann & Moosedorf 2012"),
    rockSourceLink = ifelse(LTER == "MCM", yes = "-",
                            no = "https://doi.pangaea.de/10.1594/PANGAEA.788537"),
    coverSource = case_when(
      LTER == "AND" ~ "2019 NLCD Continental US (manual bounding box via web app)",
      LTER == "ARC" ~ "2016 NLCD Alaska",
      LTER == "GRO" ~ "International Geosphere Biosphere Programme (IGBP) Land Cover Classification",
      LTER == "HBR" ~ "2019 NLCD Continental US (manual bounding box via web app)",
      LTER == "KRR" ~ "2019 NLCD Continental US (manual bounding box via web app)",
      LTER == "LMP" ~ "2019 NLCD Continental US (manual bounding box via web app)",
      LTER == "LUQ" ~ "2001 NLCD Puerto Rico",
      LTER == "MCM" ~ "Expert knowledge",
      LTER == "NWT" ~ "2019 NLCD Continental US (manual bounding box via web app)",
      LTER == "Sagehen" ~ "2019 NLCD Continental US (manual bounding box via web app)",
      LTER == "UMR" ~ "2019 NLCD Continental US (full)",
      T ~ as.character(LTER) ),
    coverSourceLink = case_when(
      LTER == "AND" ~ "https://www.mrlc.gov/viewer/",
      LTER == "ARC" ~ "https://www.mrlc.gov/data/nlcd-2016-land-cover-alaska",
      LTER == "GRO" ~ "https://www.usgs.gov/centers/eros/science/usgs-eros-archive-land-cover-products-global-land-cover-characterization-glcc",
      LTER == "HBR" ~ "https://www.mrlc.gov/viewer/",
      LTER == "KRR" ~ "https://www.mrlc.gov/viewer/",
      LTER == "LMP" ~ "https://www.mrlc.gov/viewer/",
      LTER == "LUQ" ~ "https://www.mrlc.gov/data/nlcd-2001-land-cover-puerto-rico",
      LTER == "MCM" ~ "-",
      LTER == "NWT" ~ "https://www.mrlc.gov/viewer/",
      LTER == "Sagehen" ~ "https://www.mrlc.gov/viewer/",
      LTER == "UMR" ~ "https://www.mrlc.gov/data/nlcd-2019-land-cover-conus",
      T ~ as.character(LTER) ) ) %>%
  # And clarify the meaning of the original 'dataSource' columns
  rename(shapeSource = dataSource) %>%
  rename(shapeSourceLink = dataSourceLink) %>%
  # And move all of the source columns together
  relocate(ends_with('Source'), .after = everything()) %>%
  relocate(ends_with('SourceLink'), .after = everything())

# Check contents
names(combo)

# Check McMurdo fix
combo %>%
  filter(LTER == "MCM") %>%
  select(uniqueID, major_cover, major_rock, coverSource, rockSource)
## Looks great!

# Let's actually split the source information into a separate dataframe
bib <- combo %>%
  # Get just the desired columns
  select(LTER, contains('Source', ignore.case = F)) %>%
  # And move them around
  relocate(contains('rock'), .after = LTER) %>%
  relocate(contains('cover'), .after = LTER) %>%
  relocate(contains('shape'), .after = LTER) %>%
  # And strip to just one row per LTER
  unique() %>%
  # Change the McMurdo shape source info to latest word
  mutate(
    shapeSource = ifelse(LTER == "MCM",
                         yes = "No shapefile because glacial drainage likely more important than topography",
                         no = shapeSource),
    shapeSourceLink = ifelse(LTER == "MCM", yes = "-", no = shapeSourceLink) )

# And remove sources from the 'actual' data
actual <- combo %>%
  select(-contains('Source', ignore.case = F))

# Check the contents of the bibliography file
bib

# And make sure only intended columns were dropped from data object
setdiff(names(combo), names(actual))

# Export both files to share with the working group
write.csv(actual, file = "extracted-data/SilicaSites_litho_and_landcover.csv",
          na = '', row.names = F)
write.csv(bib, file = "extracted-data/SilicaSites_data_source_info.csv",
          na = '', row.names = F)

# (Further) simplify cover categories
simp <- combo %>%
  # Remove unneeded-columns
  select(LTER, stream, uniqueID, starts_with('cover_')) %>%
  # pivot to long format
  pivot_longer(cols = starts_with('cover_'),
               names_to = "cover_categories",
               values_to = "percents") %>%
  # Drop "cover_" prefixes
  mutate(cover_categories = gsub("cover_", "", cover_categories)) %>%
  # Streamline remaining categories
  mutate(cover_categories = case_when(
    cover_categories == "deciduous_forest" ~ "forest",
    cover_categories == "deciduous_needleaf_forest" ~ "forest",
    cover_categories == "evergreen_forest" ~ "forest",
    cover_categories == "mixed_forest" ~ "forest",
    cover_categories == "cultivated_crops" ~ "impacted",
    cover_categories == "low_medium_intensity_developed" ~ "impacted",
    cover_categories == "pasture_hay" ~ "impacted",
    cover_categories == "grassland" ~ "shrub_grass",
    cover_categories == "shrubland" ~ "shrub_grass",
    cover_categories == "barren_land" ~ "barren",
    cover_categories == "open_water" ~ "water",
    cover_categories == "wetland" ~ "wetland",
    T ~ as.character(cover_categories) ) ) %>%
  # Add the "cover_" prefix back on
  mutate(cover_categories = paste0("cover_", cover_categories)) %>%
  # Make it a dataframe rather than a list tibble
  as.data.frame() %>%
  # Group by everything except cover category info
  group_by(LTER, stream, uniqueID, cover_categories) %>%
  # Summarize through new categories
  summarise(percents = sum(percents, na.rm = T)) %>%
  # Pivot to wide format
  pivot_wider(names_from = cover_categories,
              values_from = percents) %>%
  # Bring rocks back in
  left_join(y = actual %>%
              select(LTER, stream, uniqueID,
                     starts_with('rocks_')),
            by = c("LTER", "stream", "uniqueID"))

# Examine that
names(simp)

# Export this to for later use
write.csv(simp,
          file = "extracted-data/SilicaSites_simplified_litho_and_landcover.csv",
          na = '', row.names = F)

# End ----
