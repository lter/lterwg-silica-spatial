---
title: "Metajam Example"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  list(
    echo = TRUE,
    collapse = TRUE)
  )
```

Suppose we have have a link to a dataset in a data repository and we want to import it into our R session. For this example, we'll use the Stream water chemistry data for Green Lake 1, 1985-2010: https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-nwt.107.10

The `metajam` package gives us a convenient way to get the data (and the metadata) into our `R` session.

#### Step 1: Install/load the metajam package
```{r, message= FALSE}
#devtools::install_github("NCEAS/metajam")
library(metajam)
library(tidyverse) # for convenience
library(here) # for file path management
library(stringdist) # for first pass of naming-matching
library(vctrs) # for joining tables
library(readxl) # for reading the template excel sheet
```

#### Step 2: Find the link to the dataset
Go to the web address for the dataset and find the download button for the data. In general, right-clicking this and clicking "Copy Link" should do the trick. 

```{r, out.width= "50%", echo= FALSE}
include_graphics(path = here("metajam_example", "metajam_dataset_link.png"))
```

In our case this link is: https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-nwt.107.10&entityid=aef605d3ab0fba82ec95f665ff4b066b

#### Step 3: Choose where you want the files to be saved
In our case, we'll just put it into the metajam_example folder.
```{r}
#eg desired_path_to_data <- "~/Desktop"
desired_path_to_data <- here("metajam_example")
```


#### Step 4: Download the data by pasting the link you just copied
```{r}
# this will download the data into a folder and save the path to that folder
downloaded_data <- download_d1_data("https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-nwt.107.10&entityid=aef605d3ab0fba82ec95f665ff4b066b", path = desired_path_to_data)
```

#### Step 5: Now read in the data (with all the metadata)
```{r, message = FALSE}
my_data <- read_d1_files(downloaded_data)
```


Let's take a look at what we just read into `R`:
```{r}
summary(my_data)
```

It's a list of 4 dataframes! With this, everything we need is inside our `R` environment. 

#### Taking a deeper look at each of these dataframes

The dataset of interest:
```{r}
my_data$data
```

It's a good idea at this point to check that the column types are all correct. For instance, we notice that the `ANC` column is a character vector, rather than numeric. If we look a little deeper at this column, we find that this is because there are a few entries that say "NP" instead of a number.
```{r}
my_data$data %>%
  filter(ANC == "NP")
```


__Question__: What does NP mean in the `ANC` column?

To answer this, we can look at the attribute metadata that came with this dataset, which looks like this:
```{r}
my_data$attribute_metadata
```

To answer our question, we can just pull out the part of the dataframe we're interested in:
```{r}
my_data$attribute_metadata %>%
  filter(attributeName == "ANC") %>%
  select(attributeName, attributeDefinition)
```
So this tells us that NP means "not performed"! We can now deal with this column however is most appropriate for our analysis (for example, we can convert the NP into missing values and make the column numeric)

__Question__: What are the exact coordinate boundaries of the dataset?  
For this, we can look at the summary metadata:
```{r}
my_data$summary_metadata
```

To answer our question, we just subset the data we're interested in:
```{r}
# pull out all of the rows where name contains "BoundingCoordinate"
my_data$summary_metadata %>%
  filter(str_detect(name, "BoundingCoordinate"))
```

These are just a few examples of the kinds of questions we can answer with the metadata! Hopefully this gives you enough to speed up the data importing stage and get started with your analysis.

-------

_Note_: There is also a `factor_metadata` dataframe inside `my_data`. This dataset doesn't contain any factor variables, so `factor_metadata` is an empty dataframe that can be ignored. 

For more information, see the documentation and articles here: https://nceas.github.io/metajam/





--------

## Formatting data
Following the format of `stream_data_template.xlsx`, let's first import the template to see what columns we need
```{r}
# Read in the template.
# the template isn't filled out, so column types are all over the place.
  # I'm just going to make every text except date, so we can see different ways to change column type
template <- read_excel(here("metajam_example", "Stream_Data_Template.xlsx"), 
                               sheet = "Raw Data",
                               col_types = "text"
                               ) %>%
  mutate(`Sampling Date` = as.Date(`Sampling Date`))

# The new data
greenlake_data <- my_data$data
```


Next, let's create a new dataframe with the same number of rows as our new data

```{r}
# Create empty dataframe shell with the right column names.
formatted_df <- template %>% 
  slice(0)
```

Now we need to match the column names between the two tables. To do this, we'll try to make a lookup-table with the `template` column being the desired column names, and `greenlake` being the names from the new data we just downloaded.  

We'll start this lookup table by using the `amatch()` function from the `stringdist` package to fuzzy-match the two sets of names. The code below looks at each template name, and tries to find a match using the column names of `greenlake_data`. If it cannot find a close match, it will return `NA`
```{r}
# Start by matching by closest name as a first pass. 
  # Note that we match the lower case names
  # Note that the weight i= 0.1 says that we will be more likely to match if greenlake = template + extra
(fuzzy_match <- tibble(template = names(template)) %>%
   mutate(greenlake = names(greenlake_data)[amatch(tolower(template), tolower(names(greenlake_data)), 
                                                   maxDist = 1, weight = c(d=1,i=0.1,s=1,t=1))])
)
```


This was an OK first pass, but we can see that there's still a lot of blanks (eg Site/Stream Name) didn't find a match, but there's also some missed matches (eg DIC,TKN, and PON were matched up with DOC, TN, and PN respectively). 

To see if we can find a true match, we can look into the name descriptions using the below code.
```{r}
# To see if we can match up the other columns, check the documentation
my_data$attribute_metadata %>% 
  select(attributeName, attributeDefinition) 

```


Now that we have a better idea of which columns match up, we can fill in the missing / incorrect entries in our table. For example, the code below matches "Site/Stream Name" to "local_site" , matches "Sampling Date" with "date", and turns the incorrect matches into `NA`
__Note:__ The last line `TRUE ~ greenlake` tells `R` to keep the values from the `fuzzy_match` unless you overwrite them here. E.g without that last line, "LTER" and "Time" (two of the correctly matched columns) would be overwritten as `NA`.
```{r}
# Fill in the columns that didn't match, and correct the wrongly corrected matches
# continue in the same way until you've filled out everything you could.
  # Note: NA_character_ is just NA but of the character type.
(lookup_table <- fuzzy_match %>%
  mutate(greenlake = case_when(
    template == "Site/Stream Name" ~ "local_site",
    template == "Sampling Date" ~ "date",
    template %in% c("DIC", "TKN", "PON") ~ NA_character_,
    TRUE ~ greenlake
    )
    )
)
```

Once the lookup table is as filled out as possible, we can remove all rows that still had no match (which we represented by `NA` in the `greenlake` column):
```{r}
lookup_table <- lookup_table %>%
  filter(!is.na(greenlake))
```

Now we can write a function to use the lookup table
```{r}
#' @param name is a string from the greenlake column of the lookup table
# Maybe have to do a little more work to deal with ordering, since rename_at should be vectorized.
find_match<- function(name){
  lookup_table %>%
    filter(greenlake == name) %>%
    pull(template)
}
```

Now we can subset and rename the columns of the greenlake data that appear in the lookup table
```{r}
greenlake_data_of_interest <- greenlake_data %>%
  select_at(vars(lookup_table$greenlake), find_match) 
```


Finally, we can join this table to the the template format using the `vec_rbind` function from the `vctrs` package, to get the desired result.
```{r}
# combine the dataframes now that the names match up
# note: .ptype says to use the column types of greenlake_data_of_interest.
  # can switch this to formatted_df if you want to take column types from the template.
(final_output <- formatted_df %>%
  vec_rbind(greenlake_data_of_interest, .ptype = template)
)
```






